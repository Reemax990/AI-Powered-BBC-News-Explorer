import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline
import joblib
from deep_translator import GoogleTranslator

#.venv\Scripts\activate >> for term
#streamlit run app.py
def translate_if_arabic(text):
    try:
        # Ù„Ùˆ Ø§Ù„Ù†Øµ ÙÙŠÙ‡ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ© ÙŠØªØ±Ø¬Ù…
        if any("\u0600" <= c <= "\u06FF" for c in text):
            return GoogleTranslator(source='auto', target='en').translate(text)
        return text
    except Exception:
        return text 
# =====================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# =====================
# Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ (Multilingual MiniLM)
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

# Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ØªÙ„Ø®ÙŠØµ (Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙ (joblib) Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§
clf = joblib.load("classifier_model.pkl")      # LR model
vectorizer = joblib.load("tfidf_vectorizer.pkl")

# =====================
# Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =====================
# Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ù…Ø­Ø¶Ø±Ø© embeddings Ù…Ù† Ù‚Ø¨Ù„
import pandas as pd
df = pd.read_csv("bbc-news-data.csv", encoding="latin-1")
embeddings = model.encode(df["content"].tolist(), convert_to_tensor=True)

# =====================
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# =====================
st.title("AI-Powered BBC News Explorer")

# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ
st.header("ğŸ” Intelligent Search")
query = st.text_input("Ø§ÙƒØªØ¨ Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ù‡Ù†Ø§:")
if query:
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, embeddings)[0]
    top_idx = similarities.argsort()[-3:][::-1]
    results = df.iloc[top_idx][["title", "category", "content"]].copy()
    results["similarity"] = similarities[top_idx]

    for _, row in results.iterrows():
        st.subheader(f"{row['title']} ({row['category']})")
        st.write(row["content"][:300] + "...")
        st.caption(f"Similarity Score: {row['similarity']:.4f}")

        if st.button(f"ØªÙ„Ø®ÙŠØµ {row['title']}"):
            summary = summarizer(row["content"], max_length=130, min_length=30, do_sample=False)
            st.success(summary[0]['summary_text'])

# Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¢Ù„ÙŠ
st.header("ğŸ“ Auto Classification")
new_text = st.text_area("Ø§ÙƒØªØ¨ Ù†Øµ Ø¬Ø¯ÙŠØ¯ Ù„Ù„ØªØµÙ†ÙŠÙ:")
if st.button("ØµÙ†Ù‘Ù Ø§Ù„Ù†Øµ"):
    if new_text.strip() != "":
        processed_text = translate_if_arabic(new_text)
        new_embedding = vectorizer.transform([processed_text])
        prediction = clf.predict(new_embedding)[0]
        st.success(f"âœ… ØªÙ… ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ ØªØ­Øª: **{prediction}**")
    else:
        st.warning("âš ï¸ Ø±Ø¬Ø§Ø¡Ù‹ Ø§ÙƒØªØ¨ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹.")
